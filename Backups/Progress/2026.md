# 2026 Working Progress  
## By 2026/1/7  
* 继续调基于本地LLM的MCP Client（Flag回收）  
    * 优化**推理效率**方面的尝试
        * 摸了下`vllm`框架，期望通过该框架优化推理开销（**降低推理延时+降低显存开销**），但是没有成功：
            * `vllm`框架目前对`VLA`的支持不是很好，参考[vllm官方vla示例](https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/vision_language.py)把`RoboBrain2.0`缝进去；官方示例使用的对话模板是qwen2.5-vl的，需要替换为[修改后含工具调用的jinja模板](https://github.com/YouCaiJun98/mcp-client/blob/main/template/template.jinja)。
            * `vllm`主要面向的是**高并发场景下的大模型推理服务**，它的主要目的是在生产环境中同时服务多个用户，主要的技术贡献在于**显存管理**，用`PageAttention`等技术减少碎片化显存（在这一意义上，它是设计来替代`Pytorch`显存管理方案的）；尽管vllm支持部署量化后的LLM，留有相关的量化接口，但是它并没有集成量化模块，用户须在其他框架下进行模型量化，再将模型迁移到`vllm`中。自己量化`RoboBrain2.0`不太可行，因为需要利用该模型的训练数据做量化参数校正，已在`RoboBrain2.0`官方repo提了[issue](https://github.com/FlagOpen/RoboBrain2.0/issues/31)，希望开放量化后的模型，暂无回复。
                * 1/6 update: 在HF上发现了`RoboBrain2.0`的量化版本，分别是[RoboBrain2.0-7B-W8A16](https://huggingface.co/BAAI/RoboBrain2.0-7B-W8A16)与[RoboBrain2.0-7B-FP8](https://huggingface.co/BAAI/RoboBrain2.0-7B-FP8)。
            * 综上，`vllm`的设计初衷不是单模型推理优化/加速，没能实现利用`vllm`优化推理。
        * 利用上述`RoboBrain2.0`官方提供的量化后checkpoint做测试：
            * 使用[RoboBrain2.0-7B-W8A16](https://huggingface.co/BAAI/RoboBrain2.0-7B-W8A16)版本，模型加载后显存占用由16GB+变化至\~10GB，**推理速度明显降低**，由\~10s变为\~1min，且推理时显存增量明显（增量由\~300M变为\~3000M），推理质量未见退化；
            * 使用[RoboBrain2.0-7B-FP8](https://huggingface.co/BAAI/RoboBrain2.0-7B-FP8)版本，模型加载后显存占用由16GB+变化至\~10GB，推理速度也有明显下降，由 \~10s变为\~20s，但显存无明显增长；
            * 原因似乎是，`transformer`的默认pipeline会把W8反量化回W16，造成额外推理时间和显存开销；量化后模型应搭配特定的优化核使用。
    * **技术路线备份**方面的尝试
        * 评估了不同服务框架下LLM / VLM调用工具(tools)的方法，结论是**对话模板均需手动编写、消息填装与回复解析均需手动实现、多轮交互与工具调用均需手动编排**：
            * 参考`vllm`提供的[官方示例chat_with_tools](https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/chat_with_tools.py)，该框架下LLM调用工具的局限有：
                1. 工具的声明和调用方法**没有按照MCP标准**，需要手工进行message填装和工具声明；
                2. `vllm`的LLM chat pipeline对应的对话模板**完全依赖于对应LLM官方提供的模板，无通用模板**。遇到`RoboBrain2.0`这样的样例，仍需要手工编写含有工具调用的对话模板；
                3. `vllm`中LLM chat pipeline**不含流控制**，在多轮工具调用的场景下，依赖对话模板汇总对话上下文历史 + 手工编写规则判断工具调用循环是否结束。
            * 通过`openai api`实现与本地端口serve的LLM（通过`vllm`实现）进行对话，发现该方案也符合上述结论描述（需要自定义模板、交互前后处理需要手工写、无任务/工具调用编排）。
            * 尝试通过`ollama`启动本地LLM服务，它的产品定位类似于`huggingface`（但仅支持`ollama`官方提供的部分LLM）的model hub + （无模型/推理优化）vllm。发现`ollama`对本地自定义LLM的支持比较差，没找到现成的例子（似乎可以serve `GGUF`格式的[HF模型](https://huggingface.co/Mungert/RoboBrain2.0-7B-GGUF)），轻量化的设计让它更适合新手快速上手，似乎不太适合基于此进行拓展开发。
        * 对于多轮交互与工具调用的编排，或许可以尝试`LangChain`&`LangGraph`。
* 尝试将本地Robobrain2.0 MCP client接入`ros-mcp-server`
    * 
