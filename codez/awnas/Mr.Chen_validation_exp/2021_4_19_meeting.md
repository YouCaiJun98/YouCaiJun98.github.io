# 2021/4/结果反馈与讨论记录  

2021/4/30  

## XNOR-Res18 Finetune  
数据采集方法：后 10 epoch 算术平均  
| Width  | Weight Decay | Training Loss | Training Accuracy | Validation Accuracy |
|:------:|:------------:|:-------------:|:-----------------:|:-------------------:|
|   96   |      0       |   1.6725e-3   |      0.9995       |       0.9257        |
|  192   |      0       |    0.04267    |      0.9847       |       0.9019        |
|  192   |     1e-4     |    0.1164     |      0.9599       |       0.9042        |
|  192   |     5e-4     |    0.6245     |      0.7863       |       0.6048        |
|  192   |     5e-5     |    0.05670    |      0.9810       |       0.9152        |
|  192   |     1e-5     |    0.02194    |      0.9921       |       0.9121        |
|  192   |     5e-6     |    0.02111    |      0.9929       |       0.9178        |
|  192   |     3e-6     |    0.02237    |      0.9925       |       0.9080        |
|  192   |     1e-6     |    0.02861    |      0.9904       |       0.9069        |
| 256^1^ |      0       |    0.1364     |      0.9520       |       0.8583        |
| 256^2^ |     1e-5     |    0.4476     |      0.8434       |       0.7964        |
* 1. The bs of the case *init channel = 256* is 64.  
* 2. bs = 64, lr = 2e-3.   

## NES Applied to Naive MLP  
### Pseudo code:  


* 初始化目标模型target model(直接初始化weights)、评估模型evaluation model(认为是Guassian, 初始化每个参数对应分布的μ和σ)
* 在同一个batch内(same input)：
    * 对于评估模型中的每一个参数(weight):
        * 采样M次(M对应population，就是MC sample的数量):  
            * 每次采样后，计算fitness，计算log-derivatives ${\nabla}_\theta log π(\mathbf z_k|θ)
$
        * 根据M次采样的结果，计算${\nabla}_\mu J$ 和 ${\nabla}_\sigma J$ ，计算Fisher info matrix(简单起见单独计算μ和σ的两个F值)  
        * 更新${\mu}$和${\sigma}$


Q1: evaluate模型最后的weights是什么？能想到的：以μ为最后的weights(并量化)或者从学到的分布中采样(并量化)  
Q2: fitness用loss合适吗？按说应该表现越好reward越高，所以fitness也越高，但是loss和这个过程正好相反？  
Q3: weights 分布必须依次采样并更新，不能并行进行？  

## Experiment Results  
### Parameters & Settings 
* `num_init=3` 考虑到模型参数初始化对结果的影响很大（随机性很强），对模型进行若干次初始化，分别测试并对结果取平均。  
* `num_dim=[2, 4, 8, 16, 32]` 实验模型是一个三层MLP，每层MLP上neuron数量为`num_dim`。  
* `num_batch=10` 每次初始化模型后使用若干batch计算梯度，每一batch维度符合(160000, num_dim)，数据i.i.d.采样自N(0, 1)。  


###  Quick Check  
作图说明：  
* `lr`固定为0.1；  
* `Loss decrease` 表示loss下降的绝对值；  

三张图分别为：  
* NES采样数分别为500/5000/50000时计算出的grad用于梯度下降，与CDG grad用于梯度下降的结果对比：    

![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202104300001.png)  

* 不同的STE(vanilla/ReLU, Steep, SwishSign, Polynomial) grad梯度下降，与CDG grad用于梯度下降的结果对比：    

![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202104300002.png)  

其中不同的STE的形式为（自BinaryDuo）：  

![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202104300004.png)  

* 将STE(vanilla/ReLU)、STE(SwishSign)、NES(population = 50000)、CDG grad的下降结果在同一张图中描绘：  

![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202104300003.png)  
