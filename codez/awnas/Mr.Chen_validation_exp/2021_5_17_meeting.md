# 2021/5/19结果反馈与讨论记录  

2021/5/15  

## BinaryDuo设置实验后续  
### 搜索超参(epsilon for CDG & sigma for NES) - 1  
实验设置：  
* 沿用BinaryDuo上实验设置（2 models / gaussian data）；  
* 没有预训练，简单检查超参对CDG/NES的影响。  

困难：  
* NES的超参sigma搜索起来代价太大（花费时间太长），而CDG的超参epsilon在Toy model上搜索较快，可饱和搜索，因此希望CDG的表现持续弱于某一NES的表现（这样就不用搜索NES超参的最佳值了）。  

* **CDG lr-loss decrease plot**  
    * Dim = 8  
![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150001.png)  


    * Dim = 16  
![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150002.png)  

    * Dim = 32  
![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150003.png)  

    * Dim = 64  
![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150004.png)  

* **NES lr-loss decrease plot**  
    * Dim = 8
        Sample = 500  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150005.png)  

        Sample = 5000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150006.png)  

        Sample = 50000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150007.png)  

    * Dim = 16  
        Sample = 500  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150008.png)  

        Sample = 5000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150009.png)  

        Sample = 50000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150010.png)  

    * Dim = 32  
        Sample = 500  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150011.png)  

        Sample = 5000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150012.png)  

        Sample = 50000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150013.png)  

    * Dim = 64  
        Sample = 500  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150014.png)  

        Sample = 5000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150015.png)  

        Sample = 50000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150016.png)  

* **best loss decrease - num_dim plot**  

### 搜索超参(epsilon for CDG & sigma for NES) - 2  
实验设置：  
* 沿用BinaryDuo上实验设置（2 models / gaussian data）；  
* 进行了预训练，使用2000个(16000, num_dim)~N(0, 1)的数据将evaluate_model训练到基本收敛(loss不再出现明显下降)。  

问题：  
**NES grad似乎不能指向loss下降的方向**

* **CDG lr-loss decrease plot**  
    * Dim = 8  
![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150017.png)  


    * Dim = 16  
![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150018.png)  

    * Dim = 32  
![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150019.png)  

    * Dim = 64  
![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150020.png)  

* **NES lr-loss decrease plot**  
    * Dim = 8
        Sample = 500  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150021.png)  

        Sample = 5000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150022.png)  

        Sample = 50000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150023.png)  

    * Dim = 16  
        Sample = 500  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150024.png)  

        Sample = 5000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150025.png)  

        Sample = 50000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150026.png)  

    * Dim = 32  
        Sample = 500  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150027.png)  

        Sample = 5000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150028.png)  

        Sample = 50000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150029.png)  

    * Dim = 64  
        Sample = 500  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150030.png)  

        Sample = 5000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150031.png)  

        Sample = 50000  
        ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105150032.png)  

## Thin ResNet-18 CDG grad实验 - 1  
实验设置：  
* 标准XNOR-ResNet-18深度，但是initial channel分别设置为8/16  
* 通过STE预训练  

**ABOLISHED**  
* **Binary Weights & Binary Activation case**  
* initial channel = 8

（从输入端算起）第1层，`kernel size=8*8*3*3`  

![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105190001.png)  

（从输入端算起）第8层， `kernel size=16*16*3*3`  
![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105190002.png)  

（从输入端算起）倒数第二层， `kernel size=64*64*3*3`  

![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105190003.png)  

* initial channel = 16  

（从输入端算起）第1层，`kernel size=16*16*3*3`  

![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105190004.png)  

（从输入端算起）第8层，`kernel size=32*32*3*3`  

![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105190005.png)  

（从输入端算起）倒数第二层， `kernel size=256*256*3*3`
**Absent**  


* **Full-precision Weights & Binary Activation case**  
* initial channel = 8  

（从输入端算起）第1层，`kernel size=8*8*3*3`  

![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105190006.png)  

（从输入端算起）第8层， `kernel size=16*16*3*3`  

![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105190007.png)  

（从输入端算起）倒数第二层， `kernel size=64*64*3*3`  

![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105190008.png)  

## Stochastic Rounding  实验 - 1 Toy Model part  
* Stochastic Rounding流程：  
    * （预训练模型）
    *  输入一个input，对于evaluate model每一层的输出（Activations，A），（另一种方法？）分正负分别除以`torch.abs(torch.max(A))`/`torch.abs(torch.min(A))`，将A的取值范围映射到[-1, 1]，再对A加1除2，得到[0, 1]上的A作为prob，设定temperature进行relaxed Bernoulli采样，采样值取值范围为[0, 1]，减去0.5后通过sign得到stochastic rounding activation  
    * STE-like forward & backward  

* **Exp on raw model**  
* batch size = 640k  
    ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105190011.png)  

    ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105190012.png)  

    ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105190009.png)  

* batch size = 256  

    ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/202105190013.png)  

* **Exp on pretrained model**  
* 使用STE进行预训练，batch size = 16k，训练500step（至loss不再有明显下降）  
* batch size = 256  

    ![](https://raw.githubusercontent.com/YouCaiJun98/MyPicBed/main/imgs/20210519001.png)  


## Discussion 
Mr.Chen:  
* FP model上测试CDG/NES算对了（https://en.wikipedia.org/wiki/Rosenbrock_function）  
    * 还可能是平滑后的梯度指向的不是loss decrease的方向/**平滑本身有问题**  
    * 不找平滑之前的函数的最佳方向了/离散函数的方向 - NES max point（sample之后normalize，看哪个点下降最快）  
    
* Stochastic rounding 前传 量化的图 随机性  
* 在STE grad上做扰动，找loss下降最快的点  
* STE backprop改成随机的。  
* 

妃哥：  
* sr-STE更新后用vanilla STE/不引入随机性地看更新后loss的情况  
* 





