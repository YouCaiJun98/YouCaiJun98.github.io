# Kaleido's Personal Page 2022!  

Just A Private HomePage for Kaleido XD  
Happy New Year ðŸ¥³ðŸ¥³ðŸ¥³   

## What's up?  
### 2022/3/15 
* Read an ICIP19 SID pruning article [Architecture-Aware Network Pruning for Vision Quality Applications]().  

### 2022/3/14  
* Read a CVPRW20 NN deployment article [Deploying Image Deblurring across Mobile Devices: A Perspective of Quality and Latency]().  

### 2022/3/4  
* [TQT]Finished reading a useful QAT article[Trained quantization thresholds for accurate and efficient fixed-point inference of deep neural networks]().  
* [BAR]Read a differential pruning method [Structured Pruning of Neural Networks with Budget-Aware Regularization]().  

### 2022/2/16  
* [GhostNet]Read a effciency design article [GhostNet: More Features from Cheap Operations]().  

### 2022/1/21  
* [ZeroQ] Read a PTQ article [ZeroQ: A Novel Zero Shot Quantization Framework]().  

### 2022/1/15  
* Another social death while meeting. Accustomed.  
* Set Eva-1.  
* [FQSR]Read a good paper []().  

### 2022/1/14  
* Read an A+B article [A Layer-Wise Extreme Network Compression for Super Resolution]().  

### 2022/1/11  
* [DAQ]Read a bad mood article [DAQ: Channel-Wise Distribution-Aware Quantization for Deep Image Super-Resolution Networks]().  

### 2022/1/9  
* [PMAS]Read a bad A+B article [PAMS: Quantized Super-Resolution via Parameterized Max Scale]().  
* Delivered a proposal about Efficient LLCV.  

### 2022/1/6  
* [LQ-Net]Read a classic QAT-quantization article [LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks]().  

### 2022/1/4  
* Read an ICML15 article about QAT stochastic rounding [Deep Learning with Limited Numerical Precision]().   