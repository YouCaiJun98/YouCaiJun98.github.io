# Kaleido's Personal Page 2024!  

Just A Private HomePage for Kaleido XD  
Happy (Another) New Year ðŸ¥³ðŸ¥³ðŸ¥³   

## What's up?  

### 2024/2/2
* Waiting for the spring festival while doing nothing :)  

### 2024/3/4  
* [arxiv21] Read a fair article about NN model performance prediction [DNNAbacus: Toward Accurate Computational Cost Prediction for Deep Neural Networks]().


### 2024/3/5
* [ECCV20] Read a classic NAS article [Neural Predictor for Neural Architecture
Search]().  


### 2024/3/19  
* [CVPRW22][MAPLE-Edge] Finish reading a insightful article [MAPLE-Edge: A Runtime Latency Predictor for Edge Devices]().  

### 2024/3/22  
* [Mobisys21][nn-Meter] Read a good paper [nn-Meter: Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices]().  

### 2024/3/26  
* [ICLR20][NasBench-201] Read a classic NAS paper [NAS-BENCH-201: Extending the Scope of Reproducible Neural Architecture Search]().  

### 2024/4/25  
* [ICLR17][GCN] Read the classic GCN paper [Semi-Supervised Classification with Graph Convolutional Networks]().  

### 2024/5/14  
* [TPAMI22][GATES++] Read a NAS paper [A Generic Graph-based Neural Architecture  Encoding Scheme with Multifaceted Information]().  

### 2024/10/11  
* Read the following articles in the past few months:
    * [MLSys20][ParaDNN] Read a mlsys paper [A systematic methodology for analysis of deep learning hardware and software platforms]().  
    * [MLSys20][TQT] Read a quantization paper [Trained quantization thresholds for accurate and efficient fixed-point inference of deep neural networks]().  
        * It turns out that I have read this article before.  
    * [ICLR21][ViT] Read a classis CV backbone paper [An image is worth 16x16 words: Transformers for image recognition at scale]().  

