# Kaleido's Personal Page 2023!  

Just A Private HomePage for Kaleido XD  
Happy New Year ðŸ¥³ðŸ¥³ðŸ¥³   

## What's up?  
### 2023/9/18  
* [CVPR19][PointPillars] Read a classic 3D Detection paper [PointPillars: Fast Encoders for Object Detection from Point Clouds]().  

### 2023/9/12  
* [TC23][EcoFlow] Read half way an architecture papaer [EcoFlow: Efficient Convolutional Dataflows on Low-Power Neural Network Accelerators](). Barely understand this.  

### 2023/9/5  
* [IISWC21][STONNE] Finished reading a paper on AI accelerator simulator [STONNE: Enabling Cycle-Level Microarchitectural Simulation for DNN Inference Accelerators](https://youcaijun98.github.io/articles/Computer%20Architecture/%5BIISWC21%5DSTONNE.html).  

### 2023/8/24  
* [Algorithms23] Finished reading a suvery on AI accelerators [Recent Developments in Low-Power AI accelerators: A survey]().  

### 2023/8/22  
* [ACCESS19] Finished reading a survey scoped on hardware simulators. [A Survey of Computer Architecture Simulation Techniques and Tools]()

### 2023/6/25  
* Finished my Graduate Career!! ðŸ¥³ðŸ¥³ðŸ¥³

### 2023/6/19  
* [CVPR23][PTQ4DM] Read a PTQ diffusion article [Post-training Quantization on Diffusion Models]().  

### 2023/6/18  
* [NeurIPS23 submission][Q-DM] Read a diffusion quantization article [Q-DM: An Efficient Low-bit Quantized Diffusion Model]().  

### 2023/6/1   
* Read a classic BNN-Transformer article [BiT: Robustly Binarized Multi-distilled Transformer]().  

### 2023/5/21
* Continue to work on Proj Nine.  

### 2023/5/18
* Finished Graduation Reply! Half Way Master of Engineering!! ðŸ¥³ðŸ¥³ðŸ¥³

### 2023/4/16
* [CADyQ] Read a garbage SR quantization article [CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution]().  
* [XLSR] Read a classic light SR article [Extremely Lightweight Quantization Robust Real-Time Single-Image Super Resolution for Mobile Devices]().  

### 2023/3/13  
* [QT] Read a classic quantization article [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference]().

### 2023/2/3  
* [SR-STE] Read a training method for N:M pruning [Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch]().   